{
    "name": "Tera-sort on AWS",
    "tag": "Spark, Hadoop, Java, Scala, AWS", 
    "time": "Mar 2016",
    "slides": [
        {
            "image": "",
            "text": "<h2>Problem</h2><p>This is an individual class project for cloud computing course at IIT. The project requires external sorting for 1GB data and 1TB data. The required experiments includes shared-memory sorting on 1 AWS d2.xlarge instance for various threads (1, 2, 4, 6, 8), Hadoop sorting on 1-node and 17-nodes, Spark sorting for 1-node and 17-nodes. The 1TB data is too large to sorting inside memory, so we need to implement sorting programs that sorts utilizes disks while aiming a good performance. </p><p>I use Java 7, Scala 2.8.2, Hadoop 2.7.2, and Spark 1.6 on Amazon Linux for this assignment and the below discussion. The entire report below is based the older version of this project that requires 1TB data on d2.xlarge instances in US-east region with 3 * 2T instance HDD storage. Note the 1TB = 1e12 Bytes, 1GB = 1e9 Bytes for all the analysis below.</p><p>The complete source code is viewable at Github - <a href='https://github.com/yepinxia/tera-sort'>https://github.com/yepinxia/tera-sort</a>.</p>"
        },
        {
            "image": "images/terasort/01.png",
            "text": "<h2>Shared-Memory Sort Approach</h2><p>I researched the external sorting algorithm[1] , and how MapReduce conducts sorting (processing) large file. My approach combines these two methods, as shown in Figure 2.1. </p>"
        },
        {
            "image": "",
            "text": "<p>The program is written in Java, so I mark each process with the class name. The program execution is wrapped inside a bash script, because of the file manipulation in the end of the process. So “sortsmall.sh” and “sortlarge.sh” are the bash script to execute. If the user doesn’t want the final file concatenation, he/she shall invoke the Sorting java directly.</p><p>First, the ProdThread will read the input file and pass on the blocks to ConsThread. The size of each block is one of the parameters of the bash script. The number of ProdThread is always 1, to fully utilizes sequential read. The second step is each ConsThread will sort the data block. It then partitions the block into multiple pieces by a hash function. The partitions are folders that contains the sorted pieces coming from different ConsThread. The third step is MergeThread will merge the sorted pieces in each partition into one file chuck. If the partition is small enough (smaller than block size), then the merge will be in memory, otherwise, the second merge iteration will be performed as a k-way merge[2]. Finally, when all the file chucks are sorted and the order of the file chucks are sorted, the bash script will  execute a “cat” command to concatenate all files back into one. </p><p>In my experiments, the 1GB data block size is optimized at 20MB, and 1TB is at 8 threads 200MB on Amazon instances. We can’t take the full memory as Java heap will take some, as well as the system processes. I also experimented with different number of threads, which is a parameter to the program, and it will change the number of threads for both ConsThread and MergeThread. I will list the results in the later results section.  </p>"
        },
        {
            "image": "",
            "text": "<h2>Hadoop</h2><h3>My Hadoop Implementation</h3><p>My Hadoop sort “HSort.java” is a hadoop job that uses customized mapper, reducer, and partitioner.  The mapper “HSort$SMapper” takes input and split it to key and value. Then the partitioner “HSort$SPartitioner” will decide on the partitions from the number of reducers. The reducer “HSort$SReducer” simply outputs key and value. </p><p>There’s no “sort” per se in the job code, because Hadoop by default sort all records by key. In order to achieve a total sorted result, the custom partitioner will ensure each reducer will get a overall ordered chuck. I played with Hadoop “TotalOrderedPartitioner” for quite a while. It took more than 24 hour to sort the 1TB file on a single node, so I have to give it up, and found out it doesn’t work for the input format. So when I tested it, the file was never partitioned. Then I decided the most convenient way is to write my own partitioner. SPartitioner is a Hash partitioner based on the first character of key String. It functions the same as the hash function in Sorting.java in my own shared-memory sort program. </p><p>When executing the HSort.jar in Hadoop, note that it takes three parameters: input file, output file, and the number of reducers. I choose to set that as a parameter, because my partitioner need that to optimize the number of partitions. </p>"
        },
        {
            "image": "",
            "text": "<h3>Set up Clusters and Tuning Hadoop</h3><ol><li><p>0. Prepare an Image<br>In the downloaded hadoop configuration files, almost all are empty. So for each virtual machines in the cluster, I need to do the following. First of all, I created an IAM that contains hadoop, spark , and installed java JDK. Also, the environment variables are set in ~/.bashrc. The slave nodes are started with this image.</p></li><li><p>1. Disks<br>“hadoop-setup.sh” includes the script to execute once the virtual machine started. In order to enable the sorting on 1TB data, first I make sure each machine started with external storage (3 * 2T). Then, use RAID-0 to combine them into a single array, and make filesystem. I typically mount that RAID-0 onto ~/storage. Then I move hadoop and spark folder into the mounted disk. </p></li><li><p>2. HDFS<br>Other than the requirements, like change the master node address and etc. I also reduce the number of replication to 1 which will help the performance. One required change I discovered through practice is “hadoop.tmp.dir” has to be set to somewhere on the mounted disk, otherwise HDFS won’t be able to hold the 1TB data in the first place. I also updated “dfs.blocksize” to 256MB which slightly helps performance. I actually stuck here for quite a while until I hit a Yahoo article that suggests to increase HDFS block size on large dataset such that each mapper task takes more than 1 minute[3], because it takes about 30 seconds to initialize each mapper. I found out my 256MB block size only results in 45s execution time for each 3 thousands mappers. So I increased my block size to 1GB (512MB) with which the mappers takes about a minute. This change gave me a huge speedup.</p></li><li><p>3. Yarn Resource Configurations<br>I also increased the java heap size to 1000M in “yarn.env.sh”. I also tweaked the yarn resources - set available memory to 24G, CPU cores to 4. I increased the max allocated memory to 24G for Spark.</p></li><li><p>4. MapReduce Configurations<br>The mapper and reducer memory sizes are increased to 4096 as Cloudera suggested[4], and added java heap sizes accordingly. Because my block size is 512MB, I actually increase the mapper memory size to 4096 to avoid disk usage.  I also changed the “mapreduce.task.io.sort.mb” to 768MB which will prevent the map sort spilling to disk given a split size about 512MB. The sort I/O factor is increased to 50. Finally, for single node, I set a slow start of the reduce tasks to 0.9 because the single node application is not network bounded. The cluster node run with the default slowstart configuration.</p></li><li><p>5. Slaves<br>Once started 16 instances in AWS web console, I copied all slaves ip into etc/hadoop/slaves. Then executing “deploy.sh” will set up all the slaves node. The script update the code and configuration files in the system image. I have a “hadoop-setup.sh” in each instance that will be executed which will mount the disks and move the files etc.  </p></li></ol>"
        },
        {
            "image": "",
            "text": "<h2>Spark</h2><h3>My Spark Implementation</h3><p>At first, I chose Python as the programming language for this part, just because Python is a language I used before not like Scala, and it is much simpler than Java in this case. It’s called “sort.py”. My program takes 4 parameters: input file, output file, number of input workers(partition), and number of output workers(partition). The sorting is done by “sortByKey()” method. The python program is simply formatting the file, splitting key and values, mapping input, sorting, and output. The whole program is surprisingly short about 30 lines, compared to the Hadoop job, though it is partly because of the programming language I choose. More importantly, I think Spark is overall easier to use. However, the performance is very poor. I thought it may be my Spark configuration problem, but then I suspect it is due to the program I wrote. </p><p>Then, I wrote another Spark sorting program using Scala with the same approach even the same APIs. It turned out the performance is 2 times faster than Python version. I couldn’t find any articles online to endorse my comparison, but the Scala results makes more sense to me. When executing, Python code has one more stage, and the stages take longer, especially the saveAsTextFile API takes significant time. I further improved this version by implementing a custom Hash partitioner using Scala, which reduced 1 more stage. Now it needs only two stages, map and saveAsTextFile. The sorting is included in the second stage, using an API called “repartitionAndSortWithinPartitions”. This updated program has an identical flow compared Hadoop MapReduce implementation above, but of course, with fewer lines. But I found out it is extremely unwise to have fewer reduce tasks for Spark program. Because the map output aren’t sorted[6], the entire burden of sorting is in the reduce phase. So each worker needs at least 4 times the size of file for their memory in order to avoid spilling to the disk. So the final Spark program uses sortByKey API which is quite an equivalent to TotalOrderPartitioner in Hadoop. It samples the input first, and then partition accordingly.</p><p>The results in later section for Spark are all from Scala program. </p>"
        },
        {
            "image": "",
            "text": "<h3>Set up Clusters and Tuning Spark</h3><ol><li><p>0. Install<br>At first, I tried to set up Spark cluster by the ec2 script, but I found out that script doesn’t really serve my purposes. First, it can’t start a single node Spark cluster. The number of slave must be set larger than 0. Secondly, I don’t think it mount the external storage. Therefore, the instances may not have enough space.</p><p>So, I just install Spark on top of my Hadoop instance. It turns out not complicated at all due to the preparing works, e.g. mount disk, adjust Hadoop settings, are done when I set up Hadoop. They also share the same HDFS, so that I don’t need to copy my local file into HDFS twice as that takes more than an hour. </p></li><li><p>1. Path<br>The Spark configuration file I changed is “spark-env.sh”. I setup the “HADOOP_CONF_DIR” to my Hadoop configuration file location. Later after I tried to process the 1TB data, it fails at stage 2. So I figured out I should also change “SPARK_LOCAL_DIRS” to a local folder in the mounted disk.</p></li><li><p>2. Memory<br>I also updated spark-defualt.conf. I set “spark.driver.memory” to 23g and “spark.executor.memory” to 5G and set “—num-executors” to the number of cores, so that Spark can better utilize the 30G memory of Amazon instance.</p></li><li><p>3. Slaves<br>I also pasted the ip addresses into slaves file in 'spark_path/conf'. When setup 16-node cluster, the “deploy.sh” will take care of Spark configurations as well. So nothing further needed. </p></li></ol>"
        },
        {
            "image": "images/terasort/02.png",
            "text": "<h2>Evaluation and Analysis</h2><h3>Shared-Memory Threading</h3><table><thead><tr><th>Number of Threads</th><th>1</th><th>2</th><th>4</th><th>8</th></tr></thead><tbody><tr><td>1GB Exe. Time (second)</td><td>32.524</td><td>22.331</td><td>18.608</td><td>17.858</td></tr><tr><td>1TB Exe. Time (minute)</td><td>1732.68</td><td>1210.48</td><td>973.47</td><td>937.22</td></tr><tr><td>1GB adjusted (second)</td><td>31.524</td><td>22.331</td><td>17.608</td><td>16.858</td></tr><tr><td>1TB adjusted (minute)</td><td>1652.68</td><td>1130.48</td><td>893.47</td><td>857.22</td></tr><tr><td>1GB Throughput(MB/sec)</td><td>31.72</td><td>46.88</td><td>56.79</td><td>59.32</td></tr><tr><td>1TB Throughput(MB/sec)</td><td>10.08</td><td>14.74</td><td>18.65</td><td>19.44</td></tr></tbody></table><p>Chart 5.1 Results on various threads for shared-memory program</p>"
        },
        {
            "image": "",
            "text": "<p>Chart 5.1 shows results from shared-memory program on single node. The number of threads only affects ConsThread and MergeThread. ProsThread is always just 1 thread. It asynchronously read the big input file into small blocks that ConsThread takes to sort. </p><p>At first I took the final concatenation time into count. So the un-adjusted times include those. I measure the rough amount of time to concatenate files for 1GB and 1TB and subtract those from the original measurement, about 1 second for 1GB file and 80 minutes for 1TB file. Figure 5.2 shows the trend of above shared-memory program with different number of thread. It shows an exponentially decrease trend from 1 to 8. The running time of 4 threads and 8 threads are almost the same. With the d2.xlarge instance type, I expect 4 threads should have a better results. I suspect the results here is due to my parallel threads aren’t doing the same operation at the same time. For example, MergeThread reads from files, merges files, and writes back to disk. So I may have 4 threads sorting while the others writing to disk which saves time.</p><p>The 8-thread out-performed all others with a slight enhancement compared to 4-thread. For the below analysis, I will use the result from adjusted 8 threads as baseline. </p>"
        },
        {
            "image": "images/terasort/03.png",
			"text": "<h3>Single Node</h3><table><thead><tr><th>Data Size</th><th>1GB (second)</th><th>1GB Throughput(MB/sec)</th><th>1TB (minute)</th><th>1TB Throughput(MB/sec)</th></tr></thead><tbody><tr><td>Shared-memory</td><td>17</td><td>58.82</td><td>857.2</td><td>19.44</td></tr><tr><td>Hadoop</td><td>54</td><td>18.52</td><td>831.5</td><td>20.04</td></tr><tr><td>Spark</td><td>40</td><td>25.00</td><td>1021</td><td>16.32</td></tr></tbody></table><p>Chart 5.4 Results on Single-node sort with different programs</p>"
		},
        {
            "image": "",
            "text": "<p>Chart 5.4 shows the performance on a single node for both 1GB and 1TB data for three different approaches. </p><p>The Shared-memory program tops the 1GB single-node experiment. I think the reason is this program is specifically written for sorting, and it doesn’t have the overhead of resource manager like the other two platforms. The shared-memory program also utilizes asynchronous read that the other two doesn’t have. Hadoop is slower than Spark on this experiment, probably because Spark operate in memory whereas Hadoop still performs disk I/O. However, the total time to execute on a 1GB dataset is small, and the entire dataset can be fit into the memory. Therefore it’s a relatively simple task. </p><p>For 1TB experiment, Hadoop program is the fastest, closely followed by the shared-memory program. Spark is the slowest of the three. I think the reason may be the different approaches of sorting in Hadoop and Spark. So In Hadoop, the map phase sort its output before passed to the reducer, while in Spark, the map phase doesn’t sort, so the input of reducer aren’t sorted. That’s why in my Spark implementation I gave up the customized partitioner. Thus, Spark won’t take the advantage of HDD’s fast sequential read. The number of reducers in Spark equals to the mappers (which is 1863). In Cloudera’s blogpost[6], they actually improved Spark’s sort performance by implementing a Hadoop-like sort approach. Another issue I noticed with Spark is the API saveAsTextFile is extremely slow, which is a common percept in the industry. Spark’s performance will be greatly improved if we save the result as a Hadoop Sequence File which save the key/value pairs as serialized objects. I think that’s the reason why Spark is good for machine learning and data mining applications. Considering a scenario where users can repetitively read the output from a Spark job and use it for the next various tasks.</p><p>From the throughput graph, we see that Hadoop has a consistent performance over different input size, due to both experiments are I/O bounded in Hadoop, therefore, the input size is not a factor. Shared-memory program performs very well on small dataset, because of its strategy and the lack of overhead, but the 1TB case becomes I/O bounded. Spark which is able to utilize memory, out-performed Hadoop in small dataset, but has the worst performance in 1TB case, perhaps because of its sort strategy,  or the insufficient memory of these instances I use.</p>"
		},
        {
            "image": "images/terasort/04.png",
			"text": "<h3>16-Node</h3><table><thead><tr><th>Data Size</th><th>1GB (second)</th><th>1GB Throughput(MB/sec)</th><th>1TB (minute)</th><th>1TB Throughput(MB/sec)</th></tr></thead><tbody><tr><td>Hadoop</td><td>39</td><td>25.64</td><td>81.78</td><td>203.8</td></tr><tr><td>Spark</td><td>45</td><td>22.22</td><td>76.5</td><td>217.86</td></tr></tbody></table><p>Chart 5.7 Results on 16-node sort with different programs</p>"
		},
        {
            "image": "",
            "text": "<p>For 16 nodes experiments, I only tested Hadoop vs Spark. In these experiments, Hadoop and Spark performance is similar. Hadoop wins the 1GB experiment by a small margin, while Spark wins 1TB also by a small margin. </p><p>The results here actually not only because of the platform, but also a result from how the platforms are tuned. I intentionally tuned the system towards the 1TB dataset rather than 1GB, for the 1TB experiments have a larger margin to improve. For example, the HDFS block size is set to 512MB as Yahoo suggested [3]. Therefore, my 1GB input will only take 2 blocks, and thus it can’t be parallelized for 16 nodes. However, the 1TB experiments benefit from this setting a lot. The number of map tasks reduce by half to less than 2 thousand. </p><p>If we see the throughput graph, we can see there is huge enhancement from 1GB to 1TB. This is because small data set won’t benefit from multiple nodes clusters as much as big data set for the reason in the previous paragraph. The granularity of HDFS tuning is too coarse for small data set. I will analyze the comparisons of different experiments in the following section.</p><p>In the 1-node experiments, Spark is much slower to sort 1TB data compared to Hadoop, but here in 16-node, Spark surpasses Hadoop. I think it should attribute to the shuffled data size of Spark is only less than half the size of entire data, so the cost to transfer shuffled data over network is less than Hadoop. Therefore, when scaling up the clusters, Spark performs better.</p>"
		},
        {
            "image": "images/terasort/05.png",
			"text": "<h3>Speed-up</h3><table><thead><tr><th>Experiment</th><th>1 Node - 1GB</th><th>1 Node - 1TB</th><th>16 Nodes - 1GB</th><th>16 Node - 1TB</th></tr></thead><tbody><tr><td>Hadoop</td><td>0.315</td><td>1.031</td><td>0.436</td><td>10.482</td></tr><tr><td>Spark</td><td>0.425</td><td>0.840</td><td>0.378</td><td>11.205</td></tr></tbody></table><p>Chart 5.12 Speed up of experiments over the baseline of Shared-Memory sort program</p>"
		},
        {
            "image": "images/terasort/06.png",
            "text": "<p>Figure 5.10 shows all throughput results from the experiments. Shared-memory program (green line) can’t run on multiple nodes, so it only has 1 node performance results. We can see that shared-memory tops small dataset on 1 node, due to the asynchronous read/write. Hadoop and Spark performance are quite similar overall. The 1 node Hadoop / Spark programs are I/O bounded therefore their performance are close. 16 node 1GB throughput is very similar to the 1 node 1GB experiment, is because the small dataset is too small to fully exploit parallelism. We can observe a huge throughput enhancement in 16-node 1TB experiment, which benefit from scaling out workers with larger dataset.</p><p>Figure 5.11 shows the speed-up of Hadoop and Spark performance over the baseline of Shared-Memory sort. For all of the 1GB experiments the speedups are below 1. The 1TB single node speedup is close to 1, while the 1TB 16 node speed ups are above 10. Ideally, the speed-up should be 16 when the scale changes from 1 to 16. But in reality, there exists the overhead of resource manager and network communications, as well as the shuffled data are transferred in network. So the speed-ups aren’t ideal. It is more clear in Figure 5.13 that shows the speed up of Hadoop and Spark 16 node performance over its own 1 node performance on different experiments. As I discussed earlier, the 1GB dataset doesn’t benefit from the scale out. Furthermore, we can see Spark program benefit more from the scale out.  </p>"
		},
        {
            "image": "",
			"text": "<h3>Analysis on Gray Sort Benchmark</h3><p>Both Hadoop and Spark won Gray sort benchmarks in the past [7], Hadoop in 2013, Spark in 2014. The Databricks team who won the competition with Spark wrote a blog about it. They have a side-by-side comparison of the hardwares and settings they use compared to the previous Hadoop winner. They uses 207 AWS i2.8xlarge instances, whereas the Hadoop program in 2013 has a dedicated data center probably with special setups. They used 10x fewer machines and achieved 3x faster sorting speed [8]. <table><thead><tr><th></th><th>Hadoop winner in 2013</th><th>Spark winner in 2014</th><th>Myself</th></tr></thead><tbody><tr><td>CPU</td><td>2 2.3Ghz hexcore Xeon E5-2630</td><td>32 vCores - 2.5Ghz Intel Xeon E5-2670 v2</td><td>4 vCores - Intel Xeon E5-2676 v3</td></tr><tr><td>Memory</td><td>64 GB</td><td>244 GB</td><td>30.5 GB</td></tr><tr><td>Storage</td><td>12x3TB disks</td><td>8x800 GB SSD</td><td>3x2TB disks</td></tr><tr><td>Machines</td><td>dedicated data center</td><td>AWS i8.xlarge instances</td><td>AWS d2.xlarge instances</td></tr><tr><td>Number of Nodes</td><td>2100</td><td>207</td><td>16</td></tr><tr><td>Sort Speed / Node (GB/min)</td><td>0.67 GB/min</td><td>20.7 GB/min</td><td>Hadoop: 11.9 GB/min; Spark: 12.76 GB/min</td></tr></tbody></table><p>Chart 5.14 Comparison of Hardware and Setups of GraySort benchmark winners. This data comes from [7]. </p>"
		},
        {
            "image": "",
            "text": "<p>Comparing the best result I have from my experiments in Chart 5.14, which are on 16-node 1TB data, to the world competition winners, I would think my results aren’t that bad considering the limited resources I have.</p><p>It greatly surprises me both my Hadoop and Spark results surpassed 2013 winner, and I reached over 50% performance of 2014 winner with the HDD and small memory. I think this victory should attribute to platforms like Hadoop and Spark which do the heavy-liftings for people like me to operates on large dataset. The code I wrote for these implementations are minimal compared to my shared-memory program, and still they achieved excellent performance from the scalability.</p><p>Let’s also focus on the competition winners. Comparing the hardwares, Spark uses a similar CPU but with a gigantic 244GB memory where Hadoop only has 64 GB memory. Plus, Spark uses SSD when Hadoop uses HDD. Spark implementation wins at every hardware settings so far. Nevertheless, Spark exploits memory when sorting to minimize I/O. All of the above pushed the 2014 winner top others.</p>"
		},
        {
            "image": "",
            "text": "<h2>Conclusions</h2><h3>Experiment on Amazon Web Services</h3><p>All of the above experiments were run on AWS d2.xlarge instances. One thing I found out when deploying the 16-node cluster is the performance across different instances aren’t always consistent. The same operation on different instances with the same type may vary. Not all of my experiments ran multiple times. So one of the future improvement could be to run these experiment multiple times and get the average time / throughput.</p>"
        },
        {
            "image": "",
            "text": "<h3>Platforms</h3><p>My own Shared-Memory program performs very well in single node small dataset, although it can’t be scale out to a cluster. So it has very limited potentials, and requires a fair amount of efforts. With platforms like Hadoop and Spark, users can achieve a decent performance by relatively simple implementations that is also scalable.</p>"
        },
        {
            "image": "",
            "text": "<h3>Tuning the platform</h3><p>It is not that obvious in the report I actually spend more time tuning Hadoop / Spark than writing code for this project. There exists hundreds of configuration parameters, and the effect of each is implicit for a lot of cases. So most of the time, I just run the same program over and over again to see what differences would the configurations make. Additionally, the 1GB dataset is insufficient to vouch the configurations for 1TB dataset, just as single node configurations is not necessarily a good setting for 16-node cluster. The performance of programs depends dramatically on these configurations. </p>"
        },
        {
            "image": "",
            "text": "<h3>Scalability</h3><p>Given the results from Gray Sort Benchmark, and my own experiments, I predict Spark will surely out-perform Hadoop when the cluster size is large, like 100 or 1000. Spark exploits memory which increases by a lot when scaling out. </p>"
        },
		{
            "image": "",
            "text": "<h2>References</h2><ul><li>[1] External Sorting: <a href='https://en.wikipedia.org/wiki/External_sorting'>https://en.wikipedia.org/wiki/External_sorting</a></li><li>[2] K-way Merge: <a href='https://en.wikipedia.org/wiki/K-Way_Merge_Algorithms'>https://en.wikipedia.org/wiki/K-Way_Merge_Algorithms</a></li><li>[3] Apache Hadoop Best Practice Anti-Patterns: <a href='https://developer.yahoo.com/blogs/hadoop/apache-hadoop-best-practices-anti-patterns-465.html'>https://developer.yahoo.com/blogs/hadoop/apache-hadoop-best-practices-anti-patterns-465.html</a></li><li>[4] Cloudera Tuning Yarn: <a href='http://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_yarn_tuning.html'>http://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_yarn_tuning.html</a></li><li>[5] Tuning Spark:  <a href='http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/'>http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/</a></li><li>[6] Cloudera - Improving Sort Performance in Apache Spark its s Double: <a href='http://blog.cloudera.com/blog/2015/01/improving-sort-performance-in-apache-spark-its-a-double/'>http://blog.cloudera.com/blog/2015/01/improving-sort-performance-in-apache-spark-its-a-double/</a></li><li>[7] Sort Benchmark: <a href='http://sortbenchmark.org/'>http://sortbenchmark.org/</a></li><li>[8] DataBricks - Spark officially sets a new record in large-scale sorting: <a href='https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html'>https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html</a></li></ul>"
		}
       
    ]
}


