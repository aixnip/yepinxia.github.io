{
    "name": "Text Mining Experiments",
    "tag": "data mining, R", 
    "time": "Feb 2016",
    "slides": [
        {
            "image": "",
            "text": "<h2>Project Requirements</h2><p>This is an individual class project for advanced data mining course at IIT. The project requires to experiment clustering on different pre-precessed datasets. I used two data sets, Reuters (20k economics articles) and 20 news groups. </p>"
        },
        {
            "image": "",
            "text": "<h2>Reuters Dataset</h2><p>The dataset is developed and collected by Carnegie Group and Reuters. It contains news articles from Reuters, mostly form business and finance section. The original dataset is in form of machine readable files. I used the version where the documents are categorized into topics. The dataset contains two sub-folders - “test”, and “training”. First, I consolidate these files into the root folder.  </p> <p>Each document contains the content of the article only, no headers or metadata. There are 91 topics in total. One article can belong to multiple different topics. The topics are quite literal, like coffee, cocoa, rather than food which is abstract. Secondly, I therefore combined the sub-topics into more general ones. for analysis purposes. Below are a summary for the categories I used in this analysis. I de-duplicated the documents in the same category as well. </p>"
        },
        {
            "image": "",
            "text": "<table><thead><tr><th>Topic Title</th><th>Original Topics</th><th>No. Documents</th><th>No. Unique Words</th></tr></thead><tbody><tr><td>agriculture</td><td>barley, carcass, caster-oil, cocoa, coconut, coconut-oil, coffee, copra-cake, corn, cotton, cotton-oil, grain, groundnut, groundnut-oil, hog, I-cattle, lin-oil, meal-feed, oat, oilseed, orange, palm-oil, palm kernel, potato, rape-oil, rapeseed, rice, rye, sorghum, soy-meal, soy-oil, soybean, sugar, sun-meal, sun-oil, sunset, tea, veg-oil, wheat</td><td>1265</td><td>10589</td></tr><tr><td>currency</td><td>dfl, dlr, dmk, nkr, nzdlr, rand, yen</td><td>213</td><td>3977</td></tr><tr><td>economy</td><td>acq, bop, cpi, cpu, earn, gap, housing, income, instal-debt, interest, ipi, jobs, lei, money-fx, money-supply, reserves, retail, trade, wpi</td><td>8378</td><td>26352</td></tr><tr><td>energy</td><td>crude, fuel, gas, heat, naphtha, nat-gas, pet-chem, propane</td><td>698</td><td>9024</td></tr><tr><td>metal</td><td>alum, copper, gold, iron-steal, lead, nickel, palladium, silver, strategic-metal, tin, zinc</td><td>385</td><td>5956</td></tr><tr><td>raw materials</td><td>lumber, rubber</td><td>64</td><td>2319</td></tr><tr><td>transportation</td><td>ship, jet</td><td>291</td><td>5599</td></tr><tr><td>unknown</td><td>unknown</td><td>2110</td><td>13014</td></tr></tbody></table>"
        },
        {
            "image": "",
            "text": "<h2>20NewsGroup Dataset</h2><p>This dataset includes newsgroup posts labeled with different topics. The meta data includes lines, referenced emails, the author’s email, and etc. </p> <p>There are 20 total topics. Each article can belong to only one topic, but in the article, it can quote other articles in the same topic. So the articles are redundant in processing. This dataset contains 19997 documents. Each topic contains about 1000 documents, and about more than 10 thousands, sometimes about 20 thousands,  of unique words. Below I list some sampled categories. </p>"
        },
        {
            "image": "",
            "text": "<table><thead><tr><th>Topic Title</th><th>No. Documents</th><th>No. Unique Words</th></tr></thead><tbody><tr><td>alt.atheism</td><td>1000</td><td>17653</td></tr><tr><td>comp.graphics</td><td>1000</td><td>19268</td></tr><tr><td>rec.autos</td><td>1000</td><td>15646</td></tr><tr><td>sci.electronics</td><td>1000</td><td>15664</td></tr><tr><td>sci.med</td><td>1000</td><td>22150</td></tr></tbody></table>"
        },
        {
            "image": "",
            "text": "<h2>Data Preprocessing</h2><p>I selected the following topics: <br>Reuters: Agriculture, Metal<br>20NewsGroups: sci.med, comp.graphics<br><br>When I process the two groups of topics again, I got the following results</p><table><thead><tr><th>Topic Title</th><th>No. Documents</th><th>No. Unique Words</th></tr></thead><tbody><tr><td>Reuters</td><td>1083</td><td>12730</td></tr><tr><td>20NewsGroups</td><td>2000</td><td>35228</td></tr></tbody></table><p>The Reuters data, I expect, would perform well in clustering. From the previous section, we can see that 20NewsGroups has a evenly distributed number of articles, but the average unique words / document is higher than Reuters dataset. I think it’s because it has lots of noises in the header and metadata, such as the email addresses. So I don’t think 20NewsGroups data will perform well. </p><p>For Reuters data, even if the topics differ a lot, they belong to the same finance/economics section. For 20 NewsGroups data, the topic content should be distinguishable.  </p>"
        },
        {
            "image": "",
            "text": "<h2>Experiments Summary</h2><p>For each matrix, I tried K-means clustering with three different number of clusters, k = 5, 10, 20. The matrices I ran clustering experiments on includes, document-term matrix, term-document matrix, 5-dimensional term-document matrix, 10-dimensional term-document matrix, 20-dimensional term-document matrix, 50-dimensional term-document matrix, and the tf_idf weighted document matrix. </p>"
        },
        {
            "image": "",
            "text": "<h2>Analysis</h2><h3>LSA has much better clustering results</h3><p>As the between_SS/total_SS tables clearly showed in charts below(the percentage higher is better), in both dataset, reduced dimension LSA vectors shows a huge improvement on the clustering results, especially on 20NewsGroups data. </p><p>Between_SS/total_SS for different experiments of Reuters dataset</p><table><thead><tr><th>Matrix</th><th>kmeans 5 clusters</th><th>kmeans 10 clusters</th><th>kmeans 20 clusters</th></tr></thead><tbody><tr><td>Term-Doc</td><td>1.5%</td><td>2.7%</td><td>4.0%</td></tr><tr><td>5-dimensional Term-doc</td><td>56.9%</td><td>72.9%</td><td>81.3%</td></tr><tr><td>10-dimensional Term-doc</td><td>35.6%</td><td>57.6%</td><td>67.8%</td></tr><tr><td>20-dimensional Term-doc</td><td>22.0%</td><td>35.9%</td><td>52.7%</td></tr><tr><td>50-dimensional Term-doc</td><td>8.5%</td><td>17.1%</td><td>27.8%</td></tr></tbody></table><p>Between_SS/total_SS for different experiments of 20NewsGroups dataset</p><table><thead><tr><th>Matrix</th><th>kmeans 5 clusters</th><th>kmeans 10 clusters</th><th>kmeans 20 clusters</th></tr></thead><tbody><tr><td>Term-Doc</td><td>4.6%</td><td>8.7%</td><td>11.2%</td></tr><tr><td>5-dimensional Term-doc</td><td>70.6%</td><td>87.4%</td><td>91.0%</td></tr><tr><td>10-dimensional Term-doc</td><td>44.7%</td><td>61.5%</td><td>77.8%</td></tr><tr><td>20-dimensional Term-doc</td><td>31.3%</td><td>46.8%</td><td>62.7%</td></tr><tr><td>50-dimensional Term-doc</td><td>15.5%</td><td>26.5%</td><td>38.8%</td></tr></tbody></table>"
        },
        {
            "image": "images/textMining1/01.png",
            "text": "<p>The plots below shows the clustering results for 20NewsGroup matrices. We could observe a 2-dimensional pattern in the original term-doc matrix because of the number of topic I choose. The SS rate in this case is 8.7% which is very not ideal.If we reduce the dimension with SVD, and then clustering, the results will be greatly improved. All the number of clusters in this comparison is 10. The between_SS by total_SS for 5, 10, 20, 50 concepts are 87.4%, 61.5%, 46.8%, 26.5% respectively. It shows the clustering accuracy decreases as the k value in k-dimensional representation increases. It also indicates that the data points of each clusters are more concentrated towards their centroids. As we can notice in the above plots, the clusters in top left is the most unambiguous out of all. With the same total number of clusters, that’s also the plot that has the most observable clusters. Therefore, the LSA greatly improves K-means clustering results by eliminating unimportant noise. </p>"
        },
        {
            "image": "images/textMining1/02.png",
            "text": "<h3>Plot without PCA and Plot with PCA</h3><p>First when I ran experiments on Reuters dataset, I didn’t apply PCA when plotting. I found the results aren’t that obvious. After I tried PCA plotting, the clusters becomes visible. See above on the left is a plot of Reuters dataset 50-dimensional TDM without PCA, and on the right is the same data with PCA. PCA greatly improved visibility of clusters by rotating and moving the coordinate system. </p>"
        },
        {
            "image": "images/textMining1/03.png",
            "text": "<h3>The Number of Clusters in K-Means</h3><p>As the experiment result tables show, I ran all the experiments with different number of clusters in K-means. I tried 5, 10, and 20. In all cases, the between sum of square by total sum of square enhanced and the within-cluster sum of square decreases with the increase of number of clusters. So the clustering becomes more accurate. Below shows Reuters data 5-dimensional K-means clustering plot with k=5 on the left, k = 10 in the middle, and k = 20 on the right.</p>"
        },
        {
            "image": "images/textMining1/04.png",
            "text": "<h3>Tf-idf</h3><p>Within-cluster sum of squares / total sum of squares - 10clusters</p><table><thead><tr><th></th><th>DTM</th><th>TDM</th><th>Tf-idf</th></tr></thead><tbody><tr><td>Reuters</td><td>1299.65 / 1480.216</td><td>12347.08 / 12670.78</td><td>1504.046 / 1616.418</td></tr><tr><td>20NewsGroup</td><td>1835.905 / 1927.228</td><td>33273.02 / 34950.84</td><td>1933.138 / 1980.642</td></tr></tbody></table><p>Without the dimension reduction, for both dataset, the clustering on the original data performed bad. All of the between_SS/total_SS are below 20%. This is because of the noise in the data, the concepts won’t reveal. See the above table for within-cluster sum of squares / total sum of squares. It’s not necessary to show the plots here, for they don’t tell any stories.</p><p>Tf-idf approach is the inverse of LSA. It mitigates the effects of common words and strengthen the unique words while LSA eliminates the rare words. Tf-idf approach reduced the total volume of semantic space. Because some of the frequent terms now have 0 weight, and in fact the toal weighted value of all the terms decreased by the logarithm. </p><p>If we compare the results of two dataset, 20NewsGroups data points becomes even more scattered, while Reuters data points becomes somewhat concentrated towards their clusters. This is due to the header noise of 20NewsGroups data. A fair amount of unique words in 20NewsGroups that tf-idf believes can be used to cluster are header noise like email addresses, path, etc. Reuters data doesn’t have those noise, and it has a lot of common words, so this approach performs better on Reuters data. </p>"
        },
        {
            "image": "",
            "text": "<h3>Dataset</h3><p>If we compare the absolute value of within-cluster sum of squares, Reuters dataset wins on all experiments. If we compare how much the data improved when applying algorithms, 20NewsGroups performed slightly better in LSA vectors than Reuters data. See above plots for a comparison of these two datasets.</p><p>The reason why Reuters performed better over all is for the number of articles used and number of unique words the dataset contains, Reuters data has less documents, and far less unique words to begin with. Plus, Reuters data only contains the main articles without header noise I alluded to.</p>"
        },
        {
            "image": "",
            "text": "<h3>K-Concepts</h3><p>Reuters Dataset - Representative Words for Each of the first 5 concepts</p><table><thead><tr><th>Concept</th><th>top 10 representative words (in the order of frequency)</th></tr></thead><tbody><tr><td>1</td><td>'said'   'tonnes' 'year'   'will'   'market' 'sugar'  'last'   'prices' 'march'  'wheat'</td></tr><tr><td>2</td><td>'tonnes'     'said'    'wheat'      'stock'      'cocoa'      'last'       'buffer'     'grain'      'total'  'production'</td></tr><tr><td>3</td><td>'cocoa'         'stock'         'buffer'        'tonnes'        'said'          'market'        'icco'          'delegates'   'international' 'london' </td></tr><tr><td>4</td><td>'sugar'  'wheat'  'grain'  'year'   'corn'   'tonnes' 'prev'   'usda'   'total'  'dlrs'</td></tr><tr><td>5</td><td>'stocks'     'wheat'      'total'      'tonnes'     'production' 'price'      'exports'    'soybean'    'said'       'start'  </td></tr></tbody></table><p>20NewsGroup Dataset - Representative Words for Each of the first 5 concepts</p><table><thead><tr><th>Concept</th><th>top 10 representative words (in the order of frequency)</th></tr></thead><tbody><tr><td>1</td><td>'jpeg'      'image'     'file'      'format'    'images'    'available' 'will'      'also'      'color'     'version' </td></tr><tr><td>2</td><td> 'jpeg'      'data'      'graphics'  'also'      'available' 'image'     'system'    'send'      'mail'      'research' </td></tr><tr><td>3</td><td> 'health'     'cancer'     'medical'    'image'      'april'      'aids'       'patients'   'newsletter' 'number'     'page'</td></tr><tr><td>4</td><td> 'image'      'send'       'mail'       'graphics'   'data'       'processing' 'format'     'analysis'   'rayshade'   'objects'   </td></tr><tr><td>5</td><td> 'vitamin'   'will'      'health'    'among'     'cancer'    'tobacco'   'candida'   'people'    'smokeless' 'just' </td></tr></tbody></table><p>The reason why we observe a greater improvement on LSA in 20NewGroups is all Reuters data, despite the different topics, are talking about price, market, finance, and etc. They are closely interrelated to each other. If you find the representative words report, the top five concepts are quite identical agricultural topics with overlapped key words. See the above plot on the right, Reuters data clusters are loose and scattered, compared to the plot on the left where the dots are concentrated to the edges. </p><p>20NewsGroups data topics are distinguished from each other far enough. Although I have pointed out repeatedly that 20NewsGroups dataset contains header that provides noise, when LSA is performed on the data, those noise can be eliminated. See the top representative words for 20NewsGroups, they clearly belongs to the different topic, the 1, 2, 4 is for comp.graphics, and 3, 5 for sci.med. Also, if we read through the words, we can get a basic understanding for its concept, e.g.  #3 is talking about cancer and AIDS. </p>"
        },
        {
            "image": "",
            "text": "<h2>Conclusion</h2><p>LSA is a powerful tool to reveal the topics and clustering the results. LSA is also useful to reduce the noise in the raw data. It would group the data points towards the cluster centroids. Tf-idf is an approach that works on interrelated dataset. It is the inverse of LSA, and it can reduce the semantic space. I think the next step is to try LSA with Tf-idf. That may produce some interesting clusters. </p>"
        }
       
    ]
}


